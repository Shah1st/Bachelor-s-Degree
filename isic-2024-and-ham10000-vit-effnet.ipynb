{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport h5py\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom transformers import ViTModel, ViTImageProcessor\nfrom torchvision import models, transforms\nfrom sklearn.metrics import roc_auc_score, f1_score\nimport time\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nimport os\n","metadata":{"_uuid":"a4eacf68-7740-4544-970c-0f4edd2cc600","_cell_guid":"31fe1743-4b78-4c32-a6d5-f02411c0d4f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T08:30:13.896862Z","iopub.execute_input":"2024-11-13T08:30:13.897203Z","iopub.status.idle":"2024-11-13T08:30:33.275618Z","shell.execute_reply.started":"2024-11-13T08:30:13.897168Z","shell.execute_reply":"2024-11-13T08:30:33.274616Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ISICDataset(Dataset):\n    def __init__(self, df, hdf5_file, processor, model_type=\"vit\", neg_sample=0.01, pos_sample=5.0, seed=42):\n        self.hdf5_file = hdf5_file\n        self.processor = processor\n        self.model_type = model_type\n        self.seed = seed\n\n        # Apply balanced sampling\n        self.df = self._balance_sampling(df, neg_sample, pos_sample)\n\n    def _balance_sampling(self, df, neg_sample, pos_sample):\n        positive_df = df.query(\"target == 0\").sample(frac=neg_sample, random_state=self.seed)\n        negative_df = df.query(\"target == 1\").sample(frac=pos_sample, replace=True, random_state=self.seed)\n        balanced_df = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0, random_state=self.seed)\n        return balanced_df.reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        isic_id = self.df.iloc[idx]['isic_id']\n        label = torch.tensor(self.df.iloc[idx]['target'], dtype=torch.float32)\n\n        image_data = self.hdf5_file[isic_id][()]\n        image_array = np.frombuffer(image_data, np.uint8)\n        image = cv2.cvtColor(cv2.imdecode(image_array, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n\n        if self.model_type == \"vit\":\n            # For ViT, use the processor directly\n            inputs = self.processor(images=image, return_tensors=\"pt\")\n            image = inputs['pixel_values'].squeeze()\n        else:\n            # For EfficientNet, convert numpy array to PIL image and apply transforms\n              # Convert to PIL format\n            image = self.processor(Image.fromarray(image))   # Apply EfficientNet transformations\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:33.277084Z","iopub.execute_input":"2024-11-13T08:30:33.277644Z","iopub.status.idle":"2024-11-13T08:30:33.288716Z","shell.execute_reply.started":"2024-11-13T08:30:33.277609Z","shell.execute_reply":"2024-11-13T08:30:33.287657Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ISICModel(nn.Module):\n    def __init__(self, model_name=\"vit\"):\n        super(ISICModel, self).__init__()\n        self.model_name = model_name\n        \n        if model_name == \"vit\":\n            self.model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n            self.classifier = nn.Linear(self.model.config.hidden_size, 1)\n        elif model_name == \"effnet_b0\":\n            self.model = models.efficientnet_b0(pretrained=True)\n            self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, 1)\n            self.classifier = self.model.classifier[1]\n        elif model_name == \"effnet_b4\":\n            self.model = models.efficientnet_b4(pretrained=True)\n            self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, 1)\n            self.classifier = self.model.classifier[1]\n        else:\n            raise ValueError(\"Model name must be 'vit', 'effnet_b0', or 'effnet_b4'\")\n            \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        if self.model_name == \"vit\":\n            outputs = self.model(pixel_values=x)\n            x = outputs.last_hidden_state[:, 0, :]  # CLS token\n            x = self.classifier(x)\n        else:\n            x = self.model(x)\n        return self.sigmoid(x).squeeze()","metadata":{"_uuid":"4515c9b4-3369-419c-a70a-14dbc483d494","_cell_guid":"5004c062-dfab-4e21-a880-92b1524c09df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T08:30:33.289966Z","iopub.execute_input":"2024-11-13T08:30:33.290343Z","iopub.status.idle":"2024-11-13T08:30:33.329071Z","shell.execute_reply.started":"2024-11-13T08:30:33.290295Z","shell.execute_reply":"2024-11-13T08:30:33.328215Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to calculate accuracy\ndef calculate_accuracy(y_true, y_pred):\n    y_pred = torch.round(y_pred)  # Convert probabilities to binary labels\n    return (y_true == y_pred).float().mean()\n\n# Custom F1-Score calculation for binary classification\ndef calculate_f1_torch(true_labels, predicted_labels):\n    tp = (true_labels * predicted_labels).sum().float()\n    fp = ((1 - true_labels) * predicted_labels).sum().float()\n    fn = (true_labels * (1 - predicted_labels)).sum().float()\n    \n    precision = tp / (tp + fp + 1e-8)  # Adding epsilon to avoid division by zero\n    recall = tp / (tp + fn + 1e-8)\n    \n    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n    return f1.item()\n\n# Custom AUC-ROC calculation for binary classification\ndef roc_auc_torch(y_true, y_scores):\n    # Sort scores and corresponding true labels\n    desc_score_indices = torch.argsort(y_scores, descending=True)\n    y_scores_sorted = y_scores[desc_score_indices]\n    y_true_sorted = y_true[desc_score_indices]\n    \n    # Compute true positive and false positive rates\n    tps = torch.cumsum(y_true_sorted, dim=0)\n    fps = torch.cumsum(1 - y_true_sorted, dim=0)\n\n    # Add a 0 at the beginning for FPR and TPR\n    tps = torch.cat([torch.tensor([0.0], device=tps.device), tps])\n    fps = torch.cat([torch.tensor([0.0], device=fps.device), fps])\n\n    # Normalize tps and fps to get the TPR and FPR (False Positive Rate)\n    tpr = tps / tps[-1]  # True positive rate (recall)\n    fpr = fps / fps[-1]  # False positive rate\n\n    # Calculate AUC as the area under the TPR-FPR curve (trapezoidal rule)\n    auc = torch.trapz(tpr, fpr)  # Use torch.trapz to compute the integral\n    return auc.item()","metadata":{"_uuid":"69e320d3-ce13-4d59-9c95-15e002882d8a","_cell_guid":"360871ff-c93c-4d13-909a-0eec1f0c8cce","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T08:30:33.331118Z","iopub.execute_input":"2024-11-13T08:30:33.331484Z","iopub.status.idle":"2024-11-13T08:30:33.344119Z","shell.execute_reply.started":"2024-11-13T08:30:33.331448Z","shell.execute_reply":"2024-11-13T08:30:33.343316Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ModelTrainer:\n    def __init__(self, model, train_loader, val_loader, device=\"cuda\", log_interval=100):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.criterion = nn.BCELoss()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n        self.log_interval = log_interval\n\n    def train(self, epochs=5):\n        for epoch in range(epochs):\n            epoch_start_time = time.time()\n            self.model.train()\n            train_loss, train_acc, train_f1 = 0.0, 0.0, 0.0\n            true_labels, prob_preds = [], []\n\n            # Training loop with step-wise output\n            for step, (images, labels) in enumerate(self.train_loader, 1):\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = self.criterion(outputs.squeeze(), labels)\n                loss.backward()\n                self.optimizer.step()\n\n                # Collect metrics for each step\n                train_loss += loss.item()\n                accuracy = (torch.round(outputs.squeeze()) == labels).float().mean().item()\n                f1 = calculate_f1_torch(labels, torch.round(outputs).squeeze())  # Custom F1 function\n                train_acc += accuracy\n                train_f1 += f1\n                true_labels.extend(labels.cpu().numpy())\n                prob_preds.extend(outputs.detach().cpu().numpy())\n\n                # Log at specified intervals\n                if step % self.log_interval == 0:\n                    avg_loss = train_loss / step\n                    avg_acc = train_acc / step\n                    avg_f1 = train_f1 / step\n                    print(f'Epoch [{epoch+1}/{epochs}], Step [{step}/{len(self.train_loader)}], '\n                          f'Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}, F1-Score: {avg_f1:.4f}')\n\n            # Calculate epoch metrics\n            epoch_loss = train_loss / len(self.train_loader)\n            epoch_acc = train_acc / len(self.train_loader)\n            epoch_f1 = train_f1 / len(self.train_loader)\n            epoch_auc = roc_auc_torch(torch.tensor(true_labels), torch.tensor(prob_preds))  # Custom AUC function\n            epoch_time = (time.time() - epoch_start_time) / 60\n\n            print(f'End of Epoch [{epoch+1}/{epochs}] -> '\n                  f'Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, F1-Score: {epoch_f1:.4f}, '\n                  f'AUC-ROC: {epoch_auc:.4f}, Time: {epoch_time:.2f} mins')\n\n            # Validation step at the end of each epoch\n            self._validate()\n\n    def _validate(self):\n        self.model.eval()  # Set model to evaluation mode\n        val_loss, val_acc, val_f1 = 0.0, 0.0, 0.0\n        true_labels, prob_preds = [], []\n\n        with torch.no_grad():\n            for images, labels in self.val_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = self.model(images)\n                loss = self.criterion(outputs.squeeze(), labels)\n\n                val_loss += loss.item()\n                accuracy = (torch.round(outputs.squeeze()) == labels).float().mean().item()\n                f1 = calculate_f1_torch(labels, torch.round(outputs).squeeze())  # Custom F1 function\n\n                val_acc += accuracy\n                val_f1 += f1\n                true_labels.extend(labels.cpu().numpy())\n                prob_preds.extend(outputs.cpu().numpy())\n\n        # Average metrics for validation\n        val_loss /= len(self.val_loader)\n        val_acc /= len(self.val_loader)\n        val_f1 /= len(self.val_loader)\n        val_auc = roc_auc_torch(torch.tensor(true_labels), torch.tensor(prob_preds))  # Custom AUC function\n\n        print(f'Validation -> Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1-Score: {val_f1:.4f}, AUC-ROC: {val_auc:.4f}')\n        \n        def predict(self, dataloader):\n            \"\"\"Method to predict outputs on a new dataset\"\"\"\n            self.model.eval()\n            predictions = []\n\n            with torch.no_grad():\n                for images, _ in dataloader:\n                    images = images.to(self.device)\n                    outputs = self.model(images)\n                    predictions.extend(outputs.squeeze().cpu().numpy())\n\n            return predictions","metadata":{"_uuid":"77461b2f-8213-4c94-817c-79b954df7003","_cell_guid":"668a0340-ffd9-456b-8b43-3cde59dbf63e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T08:30:33.345277Z","iopub.execute_input":"2024-11-13T08:30:33.345537Z","iopub.status.idle":"2024-11-13T08:30:33.366610Z","shell.execute_reply.started":"2024-11-13T08:30:33.345508Z","shell.execute_reply":"2024-11-13T08:30:33.365709Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Configuration and data paths\n# Options: 'vit', 'effnet_b0', 'effnet_b4'\ntrain_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\ntrain_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntrain_hdf5 = h5py.File(train_hdf5_path, 'r')\ntrain_metadata = pd.read_csv(train_metadata_path)\n# Initialize datasets and dataloaders as before\n\n\n# Load metadata and perform train-validation split\ntrain_metadata = pd.read_csv(train_metadata_path)\ntrain_df_isic, val_df_isic = train_test_split(train_metadata, test_size=0.2, random_state=42)","metadata":{"_uuid":"4d3e1da6-2777-4041-815c-ab7c1e40d445","_cell_guid":"4728a640-a459-45f9-ab04-885a68060bb0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trainig_isic(model_name, epochs=5):    \n    # Dataset transformations based on model\n    if model_name == \"vit\":\n        processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n        transform = None  # Processor will handle transformations\n    else:\n        processor = transforms.Compose([\n            transforms.Resize(224 if model_name == \"effnet_b0\" else 380),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n\n    # Initialize the datasets with the split data\n    train_dataset = ISICDataset(train_df_isic, train_hdf5, processor, model_type=model_name)\n    val_dataset = ISICDataset(val_df_isic, train_hdf5, processor, model_type=model_name)\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n    # Model and Trainer with validation\n    model = ISICModel(model_name=model_name)\n\n    trainer = ModelTrainer(model, train_loader, val_loader, log_interval=100)\n    trainer.train(epochs)\n    \n    return model\n\n","metadata":{"_uuid":"b088cb12-e9f0-458b-9917-c7515e2fcdad","_cell_guid":"627f1b58-b25e-4ce4-bb25-0d45e13821cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T00:02:11.075708Z","iopub.execute_input":"2024-11-13T00:02:11.076030Z","iopub.status.idle":"2024-11-13T00:02:11.085166Z","shell.execute_reply.started":"2024-11-13T00:02:11.075997Z","shell.execute_reply":"2024-11-13T00:02:11.083913Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"isic_model_vit = trainig_isic(model_name = \"vit\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"isic_model_effnet_b4 = trainig_isic(model_name = \"effnet_b4\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T13:51:07.741615Z","iopub.execute_input":"2024-11-06T13:51:07.742068Z","iopub.status.idle":"2024-11-06T14:01:30.868099Z","shell.execute_reply.started":"2024-11-06T13:51:07.742031Z","shell.execute_reply":"2024-11-06T14:01:30.866935Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:01<00:00, 57.6MB/s]\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [100/300], Loss: 0.6378, Accuracy: 0.6400, F1-Score: 0.0478\nEpoch [1/5], Step [200/300], Loss: 0.5721, Accuracy: 0.6916, F1-Score: 0.2010\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [300/300], Loss: 0.5084, Accuracy: 0.7396, F1-Score: 0.3729\nEnd of Epoch [1/5] -> Loss: 0.5084, Accuracy: 0.7396, F1-Score: 0.3729, AUC-ROC: 0.7925, Time: 1.96 mins\nValidation -> Loss: 0.3686, Accuracy: 0.8345, F1-Score: 0.6759, AUC-ROC: 0.9037\nEpoch [2/5], Step [100/300], Loss: 0.2909, Accuracy: 0.8756, F1-Score: 0.7995\nEpoch [2/5], Step [200/300], Loss: 0.2589, Accuracy: 0.8916, F1-Score: 0.8252\nEpoch [2/5], Step [300/300], Loss: 0.2424, Accuracy: 0.9004, F1-Score: 0.8385\nEnd of Epoch [2/5] -> Loss: 0.2424, Accuracy: 0.9004, F1-Score: 0.8385, AUC-ROC: 0.9597, Time: 1.95 mins\nValidation -> Loss: 0.5545, Accuracy: 0.8229, F1-Score: 0.6467, AUC-ROC: 0.8813\nEpoch [3/5], Step [100/300], Loss: 0.1410, Accuracy: 0.9456, F1-Score: 0.9056\nEpoch [3/5], Step [200/300], Loss: 0.1278, Accuracy: 0.9525, F1-Score: 0.9211\nEpoch [3/5], Step [300/300], Loss: 0.1234, Accuracy: 0.9531, F1-Score: 0.9191\nEnd of Epoch [3/5] -> Loss: 0.1234, Accuracy: 0.9531, F1-Score: 0.9191, AUC-ROC: 0.9897, Time: 1.95 mins\nValidation -> Loss: 0.6533, Accuracy: 0.8111, F1-Score: 0.6252, AUC-ROC: 0.8974\nEpoch [4/5], Step [100/300], Loss: 0.0766, Accuracy: 0.9706, F1-Score: 0.9551\nEpoch [4/5], Step [200/300], Loss: 0.0726, Accuracy: 0.9741, F1-Score: 0.9603\nEpoch [4/5], Step [300/300], Loss: 0.0698, Accuracy: 0.9756, F1-Score: 0.9606\nEnd of Epoch [4/5] -> Loss: 0.0698, Accuracy: 0.9756, F1-Score: 0.9606, AUC-ROC: 0.9966, Time: 1.95 mins\nValidation -> Loss: 0.8163, Accuracy: 0.8204, F1-Score: 0.6296, AUC-ROC: 0.8841\nEpoch [5/5], Step [100/300], Loss: 0.0551, Accuracy: 0.9794, F1-Score: 0.9626\nEpoch [5/5], Step [200/300], Loss: 0.0528, Accuracy: 0.9797, F1-Score: 0.9633\nEpoch [5/5], Step [300/300], Loss: 0.0500, Accuracy: 0.9829, F1-Score: 0.9679\nEnd of Epoch [5/5] -> Loss: 0.0500, Accuracy: 0.9829, F1-Score: 0.9679, AUC-ROC: 0.9980, Time: 1.95 mins\nValidation -> Loss: 0.8626, Accuracy: 0.8136, F1-Score: 0.6065, AUC-ROC: 0.8983\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"isic_model_effnet_b0 = trainig_isic(model_name = \"effnet_b0\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T14:01:30.870055Z","iopub.execute_input":"2024-11-06T14:01:30.870403Z","iopub.status.idle":"2024-11-06T14:03:23.391800Z","shell.execute_reply.started":"2024-11-06T14:01:30.870367Z","shell.execute_reply":"2024-11-06T14:03:23.390644Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 70.7MB/s]\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [100/300], Loss: 0.5010, Accuracy: 0.7806, F1-Score: 0.6888\nEpoch [1/5], Step [200/300], Loss: 0.4128, Accuracy: 0.8228, F1-Score: 0.7348\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [300/300], Loss: 0.3585, Accuracy: 0.8494, F1-Score: 0.7703\nEnd of Epoch [1/5] -> Loss: 0.3585, Accuracy: 0.8494, F1-Score: 0.7703, AUC-ROC: 0.9176, Time: 0.34 mins\nValidation -> Loss: 0.4086, Accuracy: 0.8297, F1-Score: 0.6749, AUC-ROC: 0.9008\nEpoch [2/5], Step [100/300], Loss: 0.1554, Accuracy: 0.9513, F1-Score: 0.9084\nEpoch [2/5], Step [200/300], Loss: 0.1437, Accuracy: 0.9509, F1-Score: 0.9145\nEpoch [2/5], Step [300/300], Loss: 0.1374, Accuracy: 0.9510, F1-Score: 0.9170\nEnd of Epoch [2/5] -> Loss: 0.1374, Accuracy: 0.9510, F1-Score: 0.9170, AUC-ROC: 0.9876, Time: 0.34 mins\nValidation -> Loss: 0.5362, Accuracy: 0.8212, F1-Score: 0.6418, AUC-ROC: 0.8904\nEpoch [3/5], Step [100/300], Loss: 0.0735, Accuracy: 0.9794, F1-Score: 0.9669\nEpoch [3/5], Step [200/300], Loss: 0.0764, Accuracy: 0.9756, F1-Score: 0.9595\nEpoch [3/5], Step [300/300], Loss: 0.0736, Accuracy: 0.9756, F1-Score: 0.9562\nEnd of Epoch [3/5] -> Loss: 0.0736, Accuracy: 0.9756, F1-Score: 0.9562, AUC-ROC: 0.9966, Time: 0.33 mins\nValidation -> Loss: 0.7604, Accuracy: 0.8026, F1-Score: 0.5488, AUC-ROC: 0.8794\nEpoch [4/5], Step [100/300], Loss: 0.0691, Accuracy: 0.9738, F1-Score: 0.9539\nEpoch [4/5], Step [200/300], Loss: 0.0587, Accuracy: 0.9800, F1-Score: 0.9630\nEpoch [4/5], Step [300/300], Loss: 0.0616, Accuracy: 0.9790, F1-Score: 0.9603\nEnd of Epoch [4/5] -> Loss: 0.0616, Accuracy: 0.9790, F1-Score: 0.9603, AUC-ROC: 0.9971, Time: 0.33 mins\nValidation -> Loss: 0.6004, Accuracy: 0.8443, F1-Score: 0.6961, AUC-ROC: 0.8976\nEpoch [5/5], Step [100/300], Loss: 0.0464, Accuracy: 0.9838, F1-Score: 0.9710\nEpoch [5/5], Step [200/300], Loss: 0.0382, Accuracy: 0.9888, F1-Score: 0.9791\nEpoch [5/5], Step [300/300], Loss: 0.0385, Accuracy: 0.9890, F1-Score: 0.9765\nEnd of Epoch [5/5] -> Loss: 0.0385, Accuracy: 0.9890, F1-Score: 0.9765, AUC-ROC: 0.9988, Time: 0.34 mins\nValidation -> Loss: 0.7403, Accuracy: 0.8207, F1-Score: 0.6158, AUC-ROC: 0.9023\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Define paths to save each model's weights in Kaggle's working directory\nvit_path = \"/kaggle/working/isic_model_weights_vit.pth\"\neffnet_b0_path = \"/kaggle/working/isic_model_weights_effnet_b0.pth\"\neffnet_b4_path = \"/kaggle/working/isic_model_weights_effnet_b4.pth\"\n\n# Save each model's state_dict\ntorch.save(isic_model_vit.state_dict(), vit_path)\nprint(\"Model weights saved successfully in Kaggle.\")","metadata":{"_uuid":"5a2c7f5d-9c4b-47ff-a1d1-d72facbf20d9","_cell_guid":"2c3a284c-ffb5-4341-a490-048602ee19de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-06T14:03:23.394116Z","iopub.execute_input":"2024-11-06T14:03:23.394965Z","iopub.status.idle":"2024-11-06T14:03:24.101217Z","shell.execute_reply.started":"2024-11-06T14:03:23.394913Z","shell.execute_reply":"2024-11-06T14:03:24.100215Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model weights saved successfully in Kaggle.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(isic_model_effnet_b0.state_dict(), effnet_b0_path)\ntorch.save(isic_model_effnet_b4.state_dict(), effnet_b4_path)\n\nprint(\"Model weights saved successfully in Kaggle.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')  # Uncomment and adjust if loading from a file\n\n# Dictionary to map lesion types to benign or malignant\nbenign_types = ['nv', 'bkl', 'akiec', 'vasc', 'df']\nmalignant_types = ['mel', 'bcc']\n\n# Add binary target column: 0 for benign, 1 for malignant\ndf['target'] = df['dx'].apply(lambda x: 0 if x in benign_types else 1)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:33.367479Z","iopub.execute_input":"2024-11-13T08:30:33.367758Z","iopub.status.idle":"2024-11-13T08:30:33.426877Z","shell.execute_reply.started":"2024-11-13T08:30:33.367722Z","shell.execute_reply":"2024-11-13T08:30:33.426181Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define the folder paths\nfolder_1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\"\nfolder_2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n\n# Function to locate the image path\ndef get_image_path(image_id):\n    filename = f\"{image_id}.jpg\"  # Assuming images are named after image_id with .jpg extension\n    if os.path.exists(os.path.join(folder_1, filename)):\n        return os.path.join(folder_1, filename)\n    elif os.path.exists(os.path.join(folder_2, filename)):\n        return os.path.join(folder_2, filename)\n    else:\n        return None  # In case the image is missing\n\n    \ndf['image_path'] = df['image_id'].apply(get_image_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:33.427839Z","iopub.execute_input":"2024-11-13T08:30:33.428171Z","iopub.status.idle":"2024-11-13T08:30:56.258868Z","shell.execute_reply.started":"2024-11-13T08:30:33.428137Z","shell.execute_reply":"2024-11-13T08:30:56.257958Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class HAM10000Dataset(Dataset):\n    def __init__(self, df, processor, model_type=\"vit\", seed=42):\n        \"\"\"\n        Args:\n            df (DataFrame): DataFrame containing the dataset metadata.\n            processor: Preprocessing function or model-specific transform.\n            model_type (str): Model type (\"vit\" or \"efficientnet\") to determine preprocessing.\n            seed (int): Seed for reproducibility.\n        \"\"\"\n        self.processor = processor\n        self.model_type = model_type\n        self.seed = seed\n        self.df = df\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row['image_path']\n        label = torch.tensor(row['target'], dtype=torch.float32)\n\n        # Load and process the image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.model_type == \"vit\":\n            # Process for ViT model\n            inputs = self.processor(images=image, return_tensors=\"pt\")\n            image = inputs['pixel_values'].squeeze()  # Shape: [3, height, width]\n        else:\n            # Process for EfficientNet model\n            image = self.processor(Image.fromarray(image))  # Apply EfficientNet transformations\n\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:56.260137Z","iopub.execute_input":"2024-11-13T08:30:56.260514Z","iopub.status.idle":"2024-11-13T08:30:56.270097Z","shell.execute_reply.started":"2024-11-13T08:30:56.260470Z","shell.execute_reply":"2024-11-13T08:30:56.269247Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_df_ham10000, val_df_ham10000 = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:56.271263Z","iopub.execute_input":"2024-11-13T08:30:56.271602Z","iopub.status.idle":"2024-11-13T08:30:56.298201Z","shell.execute_reply.started":"2024-11-13T08:30:56.271564Z","shell.execute_reply":"2024-11-13T08:30:56.297295Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_ham10000(model_name, epochs=5):\n    # Automatically set up preprocessing based on EfficientNet model size\n    def get_efficientnet_preprocessor(model_name):\n        target_size = 224 if model_name == \"effnet_b0\" else 380\n        return transforms.Compose([\n            transforms.Resize(target_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n    # Dataset transformations based on model\n    if model_name == \"vit\":\n        processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n        transform = None  # Processor will handle transformations\n    else:\n        processor = get_efficientnet_preprocessor(model_name)\n\n    train_dataset = HAM10000Dataset(train_df_ham10000, processor, model_type=model_name)\n    val_dataset = HAM10000Dataset(val_df_ham10000, processor, model_type=model_name)\n\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n    model = ISICModel(model_name=model_name)\n\n    trainer = ModelTrainer(model, train_loader, val_loader, log_interval=100)\n    trainer.train(epochs)\n    \n    return model\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:56.302102Z","iopub.execute_input":"2024-11-13T08:30:56.302493Z","iopub.status.idle":"2024-11-13T08:30:56.311322Z","shell.execute_reply.started":"2024-11-13T08:30:56.302460Z","shell.execute_reply":"2024-11-13T08:30:56.310166Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"ham_model_vit = train_ham10000(\"vit\", epochs=2)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:30:56.312974Z","iopub.execute_input":"2024-11-13T08:30:56.313350Z","iopub.status.idle":"2024-11-13T08:36:37.413345Z","shell.execute_reply.started":"2024-11-13T08:30:56.313305Z","shell.execute_reply":"2024-11-13T08:36:37.412196Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1298b5d0e3354c58af53c9ebeec9de66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad72906c30ce40e08b56fcd016f4e0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efae827c72f54c599cdafd485895f10f"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Step [100/501], Loss: 0.3742, Accuracy: 0.8363, F1-Score: 0.2074\nEpoch [1/2], Step [200/501], Loss: 0.3446, Accuracy: 0.8462, F1-Score: 0.2655\nEpoch [1/2], Step [300/501], Loss: 0.3363, Accuracy: 0.8504, F1-Score: 0.3044\nEpoch [1/2], Step [400/501], Loss: 0.3241, Accuracy: 0.8547, F1-Score: 0.3152\nEpoch [1/2], Step [500/501], Loss: 0.3187, Accuracy: 0.8561, F1-Score: 0.3394\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"End of Epoch [1/2] -> Loss: 0.3186, Accuracy: 0.8562, F1-Score: 0.3401, AUC-ROC: 0.8571, Time: 2.58 mins\nValidation -> Loss: 0.2711, Accuracy: 0.8920, F1-Score: 0.4697, AUC-ROC: 0.9081\nEpoch [2/2], Step [100/501], Loss: 0.2314, Accuracy: 0.9106, F1-Score: 0.5747\nEpoch [2/2], Step [200/501], Loss: 0.2464, Accuracy: 0.8969, F1-Score: 0.5501\nEpoch [2/2], Step [300/501], Loss: 0.2328, Accuracy: 0.9021, F1-Score: 0.5818\nEpoch [2/2], Step [400/501], Loss: 0.2392, Accuracy: 0.8978, F1-Score: 0.5488\nEpoch [2/2], Step [500/501], Loss: 0.2374, Accuracy: 0.8990, F1-Score: 0.5520\nEnd of Epoch [2/2] -> Loss: 0.2372, Accuracy: 0.8990, F1-Score: 0.5522, AUC-ROC: 0.9277, Time: 2.58 mins\nValidation -> Loss: 0.2583, Accuracy: 0.8929, F1-Score: 0.5299, AUC-ROC: 0.9095\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"ham_model_effnet_b0 = train_ham10000(model_name = \"effnet_b0\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T14:18:05.624792Z","iopub.execute_input":"2024-11-06T14:18:05.625145Z","iopub.status.idle":"2024-11-06T14:23:19.699302Z","shell.execute_reply.started":"2024-11-06T14:18:05.625106Z","shell.execute_reply":"2024-11-06T14:23:19.698158Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [100/501], Loss: 0.4433, Accuracy: 0.8244, F1-Score: 0.1369\nEpoch [1/5], Step [200/501], Loss: 0.3897, Accuracy: 0.8359, F1-Score: 0.2068\nEpoch [1/5], Step [300/501], Loss: 0.3623, Accuracy: 0.8448, F1-Score: 0.2910\nEpoch [1/5], Step [400/501], Loss: 0.3481, Accuracy: 0.8497, F1-Score: 0.3332\nEpoch [1/5], Step [500/501], Loss: 0.3379, Accuracy: 0.8550, F1-Score: 0.3659\nEnd of Epoch [1/5] -> Loss: 0.3374, Accuracy: 0.8553, F1-Score: 0.3651, AUC-ROC: 0.8380, Time: 0.85 mins\nValidation -> Loss: 0.2641, Accuracy: 0.8877, F1-Score: 0.4733, AUC-ROC: 0.9158\nEpoch [2/5], Step [100/501], Loss: 0.2346, Accuracy: 0.8906, F1-Score: 0.5193\nEpoch [2/5], Step [200/501], Loss: 0.2377, Accuracy: 0.8925, F1-Score: 0.5695\nEpoch [2/5], Step [300/501], Loss: 0.2334, Accuracy: 0.8954, F1-Score: 0.5766\nEpoch [2/5], Step [400/501], Loss: 0.2352, Accuracy: 0.8953, F1-Score: 0.5828\nEpoch [2/5], Step [500/501], Loss: 0.2327, Accuracy: 0.8979, F1-Score: 0.5975\nEnd of Epoch [2/5] -> Loss: 0.2325, Accuracy: 0.8981, F1-Score: 0.5983, AUC-ROC: 0.9336, Time: 0.86 mins\nValidation -> Loss: 0.2850, Accuracy: 0.8902, F1-Score: 0.4641, AUC-ROC: 0.9096\nEpoch [3/5], Step [100/501], Loss: 0.1760, Accuracy: 0.9269, F1-Score: 0.6680\nEpoch [3/5], Step [200/501], Loss: 0.1681, Accuracy: 0.9344, F1-Score: 0.7043\nEpoch [3/5], Step [300/501], Loss: 0.1663, Accuracy: 0.9340, F1-Score: 0.7236\nEpoch [3/5], Step [400/501], Loss: 0.1685, Accuracy: 0.9317, F1-Score: 0.7219\nEpoch [3/5], Step [500/501], Loss: 0.1687, Accuracy: 0.9314, F1-Score: 0.7162\nEnd of Epoch [3/5] -> Loss: 0.1689, Accuracy: 0.9312, F1-Score: 0.7157, AUC-ROC: 0.9665, Time: 0.86 mins\nValidation -> Loss: 0.2304, Accuracy: 0.9135, F1-Score: 0.5799, AUC-ROC: 0.9419\nEpoch [4/5], Step [100/501], Loss: 0.1119, Accuracy: 0.9550, F1-Score: 0.7913\nEpoch [4/5], Step [200/501], Loss: 0.1139, Accuracy: 0.9553, F1-Score: 0.7955\nEpoch [4/5], Step [300/501], Loss: 0.1165, Accuracy: 0.9550, F1-Score: 0.7888\nEpoch [4/5], Step [400/501], Loss: 0.1126, Accuracy: 0.9566, F1-Score: 0.7969\nEpoch [4/5], Step [500/501], Loss: 0.1156, Accuracy: 0.9553, F1-Score: 0.7964\nEnd of Epoch [4/5] -> Loss: 0.1154, Accuracy: 0.9553, F1-Score: 0.7968, AUC-ROC: 0.9850, Time: 0.85 mins\nValidation -> Loss: 0.2156, Accuracy: 0.9130, F1-Score: 0.6288, AUC-ROC: 0.9489\nEpoch [5/5], Step [100/501], Loss: 0.0799, Accuracy: 0.9706, F1-Score: 0.8737\nEpoch [5/5], Step [200/501], Loss: 0.0776, Accuracy: 0.9712, F1-Score: 0.8806\nEpoch [5/5], Step [300/501], Loss: 0.0782, Accuracy: 0.9723, F1-Score: 0.8761\nEpoch [5/5], Step [400/501], Loss: 0.0762, Accuracy: 0.9736, F1-Score: 0.8716\nEpoch [5/5], Step [500/501], Loss: 0.0786, Accuracy: 0.9732, F1-Score: 0.8671\nEnd of Epoch [5/5] -> Loss: 0.0785, Accuracy: 0.9733, F1-Score: 0.8674, AUC-ROC: 0.9928, Time: 0.85 mins\nValidation -> Loss: 0.2312, Accuracy: 0.9180, F1-Score: 0.6631, AUC-ROC: 0.9500\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"ham_model_effnet_b4 = train_ham10000(model_name = \"effnet_b4\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:06:38.305510Z","iopub.execute_input":"2024-11-12T22:06:38.305897Z","iopub.status.idle":"2024-11-12T22:29:24.292807Z","shell.execute_reply.started":"2024-11-12T22:06:38.305853Z","shell.execute_reply":"2024-11-12T22:29:24.291620Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:00<00:00, 206MB/s]\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Step [100/501], Loss: 0.4827, Accuracy: 0.8175, F1-Score: 0.0206\nEpoch [1/5], Step [200/501], Loss: 0.4157, Accuracy: 0.8319, F1-Score: 0.0103\nEpoch [1/5], Step [300/501], Loss: 0.3914, Accuracy: 0.8337, F1-Score: 0.0465\nEpoch [1/5], Step [400/501], Loss: 0.3748, Accuracy: 0.8378, F1-Score: 0.0978\nEpoch [1/5], Step [500/501], Loss: 0.3604, Accuracy: 0.8421, F1-Score: 0.1200\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"End of Epoch [1/5] -> Loss: 0.3603, Accuracy: 0.8421, F1-Score: 0.1197, AUC-ROC: 0.7970, Time: 4.28 mins\nValidation -> Loss: 0.2977, Accuracy: 0.8702, F1-Score: 0.3891, AUC-ROC: 0.8867\nEpoch [2/5], Step [100/501], Loss: 0.2729, Accuracy: 0.8788, F1-Score: 0.3930\nEpoch [2/5], Step [200/501], Loss: 0.2801, Accuracy: 0.8747, F1-Score: 0.4215\nEpoch [2/5], Step [300/501], Loss: 0.2771, Accuracy: 0.8742, F1-Score: 0.4103\nEpoch [2/5], Step [400/501], Loss: 0.2727, Accuracy: 0.8764, F1-Score: 0.4480\nEpoch [2/5], Step [500/501], Loss: 0.2690, Accuracy: 0.8785, F1-Score: 0.4657\nEnd of Epoch [2/5] -> Loss: 0.2687, Accuracy: 0.8787, F1-Score: 0.4668, AUC-ROC: 0.9069, Time: 4.25 mins\nValidation -> Loss: 0.2568, Accuracy: 0.8841, F1-Score: 0.4457, AUC-ROC: 0.9187\nEpoch [3/5], Step [100/501], Loss: 0.2045, Accuracy: 0.9187, F1-Score: 0.6465\nEpoch [3/5], Step [200/501], Loss: 0.1994, Accuracy: 0.9178, F1-Score: 0.6481\nEpoch [3/5], Step [300/501], Loss: 0.2038, Accuracy: 0.9146, F1-Score: 0.6309\nEpoch [3/5], Step [400/501], Loss: 0.2012, Accuracy: 0.9155, F1-Score: 0.6301\nEpoch [3/5], Step [500/501], Loss: 0.2046, Accuracy: 0.9134, F1-Score: 0.6430\nEnd of Epoch [3/5] -> Loss: 0.2045, Accuracy: 0.9135, F1-Score: 0.6437, AUC-ROC: 0.9488, Time: 4.25 mins\nValidation -> Loss: 0.2296, Accuracy: 0.8981, F1-Score: 0.5876, AUC-ROC: 0.9333\nEpoch [4/5], Step [100/501], Loss: 0.1480, Accuracy: 0.9400, F1-Score: 0.7719\nEpoch [4/5], Step [200/501], Loss: 0.1499, Accuracy: 0.9400, F1-Score: 0.7693\nEpoch [4/5], Step [300/501], Loss: 0.1481, Accuracy: 0.9400, F1-Score: 0.7649\nEpoch [4/5], Step [400/501], Loss: 0.1442, Accuracy: 0.9405, F1-Score: 0.7614\nEpoch [4/5], Step [500/501], Loss: 0.1425, Accuracy: 0.9424, F1-Score: 0.7654\nEnd of Epoch [4/5] -> Loss: 0.1422, Accuracy: 0.9425, F1-Score: 0.7658, AUC-ROC: 0.9763, Time: 4.25 mins\nValidation -> Loss: 0.2723, Accuracy: 0.9061, F1-Score: 0.5559, AUC-ROC: 0.9362\nEpoch [5/5], Step [100/501], Loss: 0.0990, Accuracy: 0.9569, F1-Score: 0.8110\nEpoch [5/5], Step [200/501], Loss: 0.0903, Accuracy: 0.9637, F1-Score: 0.8162\nEpoch [5/5], Step [300/501], Loss: 0.0894, Accuracy: 0.9652, F1-Score: 0.8242\nEpoch [5/5], Step [400/501], Loss: 0.0908, Accuracy: 0.9642, F1-Score: 0.8235\nEpoch [5/5], Step [500/501], Loss: 0.0890, Accuracy: 0.9650, F1-Score: 0.8241\nEnd of Epoch [5/5] -> Loss: 0.0889, Accuracy: 0.9651, F1-Score: 0.8244, AUC-ROC: 0.9912, Time: 4.26 mins\nValidation -> Loss: 0.2723, Accuracy: 0.9120, F1-Score: 0.6249, AUC-ROC: 0.9451\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Define paths to save each model's weights in Kaggle's working directory\nvit_path = \"/kaggle/working/ham_model_weights_vit.pth\"\neffnet_b0_path = \"/kaggle/working/ham_model_weights_effnet_b0.pth\"\neffnet_b4_path = \"/kaggle/working/ham_model_weights_effnet_b4.pth\"\n\n# Save each model's state_dict\ntorch.save(ham_model_vit.state_dict(), vit_path)\nprint(\"Saved ViT\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:36:37.415224Z","iopub.execute_input":"2024-11-13T08:36:37.415562Z","iopub.status.idle":"2024-11-13T08:36:37.856121Z","shell.execute_reply.started":"2024-11-13T08:36:37.415527Z","shell.execute_reply":"2024-11-13T08:36:37.855054Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Saved ViT\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport zipfile\nfrom datetime import datetime\n\ndef zip_model_weights(model_path='ham_model_weights_vit.pth', output_dir='/kaggle/working/'):\n    # Create timestamp for unique filename\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    zip_filename = f'model_weights_{timestamp}.zip'\n    zip_filepath = os.path.join(output_dir, zip_filename)\n    \n    # Create zip file\n    with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Add the model weights file to the zip\n        zipf.write(model_path, os.path.basename(model_path))\n    \n    print(f'Model weights have been zipped to: {zip_filepath}')\n    return zip_filepath\n\n# Usage\nzip_path = zip_model_weights()\nprint(f'You can find your zipped file at: {zip_path}')","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:55:18.282778Z","iopub.execute_input":"2024-11-13T08:55:18.283678Z","iopub.status.idle":"2024-11-13T08:55:37.674085Z","shell.execute_reply.started":"2024-11-13T08:55:18.283635Z","shell.execute_reply":"2024-11-13T08:55:37.673030Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model weights have been zipped to: /kaggle/working/model_weights_20241113_085518.zip\nYou can find your zipped file at: /kaggle/working/model_weights_20241113_085518.zip\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# If you just want to download the .pth file directly without zipping\nfrom IPython.display import FileLink\nFileLink('ham_model_weights_vit.pth')","metadata":{"execution":{"iopub.status.busy":"2024-11-13T09:00:09.442931Z","iopub.execute_input":"2024-11-13T09:00:09.443805Z","iopub.status.idle":"2024-11-13T09:00:09.449888Z","shell.execute_reply.started":"2024-11-13T09:00:09.443766Z","shell.execute_reply":"2024-11-13T09:00:09.448890Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ham_model_weights_vit.pth","text/html":"<a href='ham_model_weights_vit.pth' target='_blank'>ham_model_weights_vit.pth</a><br>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"torch.save(ham_model_effnet_b0.state_dict(), effnet_b0_path)\ntorch.save(ham_model_effnet_b4.state_dict(), effnet_b4_path)","metadata":{},"outputs":[],"execution_count":null}]}